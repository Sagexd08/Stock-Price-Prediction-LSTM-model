{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis for Stock Price Prediction\n",
    "\n",
    "This notebook explores the stock price dataset and performs initial analysis to understand the data characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add parent directory to path to import modules\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Dataset\n",
    "\n",
    "First, let's load the stock price dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "data_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))), 'data')\n",
    "raw_dir = os.path.join(data_dir, 'raw')\n",
    "\n",
    "# Load stock data\n",
    "from src.data_acquisition import load_stock_data\n",
    "\n",
    "stock_data_path = os.path.join(raw_dir, 'stock_data.csv')\n",
    "stock_data = load_stock_data(stock_data_path)\n",
    "\n",
    "# Display the first few rows\n",
    "stock_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Data Exploration\n",
    "\n",
    "Let's explore the basic characteristics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the dataset\n",
    "print(f\"Dataset shape: {stock_data.shape}\")\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nData types:\")\n",
    "print(stock_data.dtypes)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(stock_data.isnull().sum())\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary statistics:\")\n",
    "stock_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Time Series Visualization\n",
    "\n",
    "Let's visualize the stock prices over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot stock prices\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for column in stock_data.columns:\n",
    "    plt.plot(stock_data.index, stock_data[column], label=column)\n",
    "\n",
    "plt.title('Stock Prices Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Correlation Analysis\n",
    "\n",
    "Let's analyze the correlation between different stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "correlation_matrix = stock_data.corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix of Stock Prices')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Return Analysis\n",
    "\n",
    "Let's calculate and analyze daily returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate daily returns\n",
    "returns = stock_data.pct_change().dropna()\n",
    "\n",
    "# Plot returns\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for column in returns.columns:\n",
    "    plt.plot(returns.index, returns[column], label=column, alpha=0.7)\n",
    "\n",
    "plt.title('Daily Returns')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Return')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Return Distribution\n",
    "\n",
    "Let's analyze the distribution of returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot return distributions\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for i, column in enumerate(returns.columns):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    sns.histplot(returns[column], kde=True)\n",
    "    plt.title(f'{column} Return Distribution')\n",
    "    plt.xlabel('Return')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Volatility Analysis\n",
    "\n",
    "Let's calculate and visualize the volatility of each stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rolling volatility (standard deviation of returns)\n",
    "window_size = 20  # 20-day rolling window\n",
    "volatility = returns.rolling(window=window_size).std() * np.sqrt(window_size)  # Annualized\n",
    "\n",
    "# Plot volatility\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for column in volatility.columns:\n",
    "    plt.plot(volatility.index, volatility[column], label=column)\n",
    "\n",
    "plt.title(f'{window_size}-Day Rolling Volatility')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Volatility')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Autocorrelation Analysis\n",
    "\n",
    "Let's analyze the autocorrelation of stock prices and returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "# Plot autocorrelation for the first stock\n",
    "plt.figure(figsize=(14, 6))\n",
    "autocorrelation_plot(stock_data.iloc[:, 0])\n",
    "plt.title(f'Autocorrelation of {stock_data.columns[0]} Prices')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot autocorrelation for returns of the first stock\n",
    "plt.figure(figsize=(14, 6))\n",
    "autocorrelation_plot(returns.iloc[:, 0])\n",
    "plt.title(f'Autocorrelation of {returns.columns[0]} Returns')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Engineering Preview\n",
    "\n",
    "Let's preview some technical indicators that we'll use for feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.feature_engineering import add_technical_indicators\n",
    "\n",
    "# For this example, let's create a dataframe with OHLCV structure\n",
    "# We'll use the first stock as Close price and generate other columns\n",
    "ohlcv_data = pd.DataFrame({\n",
    "    'Open': stock_data.iloc[:, 0].values * 0.99,  # Slightly lower than Close\n",
    "    'High': stock_data.iloc[:, 0].values * 1.02,  # Slightly higher than Close\n",
    "    'Low': stock_data.iloc[:, 0].values * 0.98,   # Slightly lower than Close\n",
    "    'Close': stock_data.iloc[:, 0].values,\n",
    "    'Volume': np.random.normal(1000000, 200000, len(stock_data))  # Random volume\n",
    "}, index=stock_data.index)\n",
    "\n",
    "# Add technical indicators\n",
    "tech_data = add_technical_indicators(ohlcv_data, include_all=True)\n",
    "\n",
    "# Display the first few rows with technical indicators\n",
    "tech_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize Technical Indicators\n",
    "\n",
    "Let's visualize some of the technical indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot price with SMA\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(tech_data.index, tech_data['Close'], label='Close Price')\n",
    "plt.plot(tech_data.index, tech_data['SMA_20'], label='SMA 20')\n",
    "plt.plot(tech_data.index, tech_data['SMA_50'], label='SMA 50')\n",
    "plt.title('Price with Simple Moving Averages')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot RSI\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(tech_data.index, tech_data['RSI_14'])\n",
    "plt.axhline(y=70, color='r', linestyle='--', alpha=0.5)\n",
    "plt.axhline(y=30, color='g', linestyle='--', alpha=0.5)\n",
    "plt.title('Relative Strength Index (RSI)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('RSI')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot MACD\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(tech_data.index, tech_data['MACD_12_26_9'], label='MACD')\n",
    "plt.plot(tech_data.index, tech_data['MACDs_12_26_9'], label='Signal')\n",
    "plt.bar(tech_data.index, tech_data['MACDh_12_26_9'], label='Histogram', alpha=0.5)\n",
    "plt.title('Moving Average Convergence Divergence (MACD)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('MACD')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Stationarity Analysis\n",
    "\n",
    "Let's check if the time series is stationary using the Augmented Dickey-Fuller test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def adf_test(series, title=''):\n",
    "    \"\"\"Perform Augmented Dickey-Fuller test\"\"\"\n",
    "    result = adfuller(series.dropna())\n",
    "    print(f'ADF Test for {title}')\n",
    "    print('ADF Statistic: %f' % result[0])\n",
    "    print('p-value: %f' % result[1])\n",
    "    print('Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print('\\t%s: %.3f' % (key, value))\n",
    "    \n",
    "    # Interpret the result\n",
    "    if result[1] <= 0.05:\n",
    "        print(\"Conclusion: The series is stationary (reject the null hypothesis)\")\n",
    "    else:\n",
    "        print(\"Conclusion: The series is non-stationary (fail to reject the null hypothesis)\")\n",
    "    print('\\n')\n",
    "\n",
    "# Test stationarity for the first stock price\n",
    "adf_test(stock_data.iloc[:, 0], title=f'{stock_data.columns[0]} Price')\n",
    "\n",
    "# Test stationarity for the first stock returns\n",
    "adf_test(returns.iloc[:, 0], title=f'{returns.columns[0]} Returns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Seasonal Decomposition\n",
    "\n",
    "Let's decompose the time series into trend, seasonal, and residual components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Perform seasonal decomposition on the first stock\n",
    "decomposition = seasonal_decompose(stock_data.iloc[:, 0].dropna(), model='additive', period=30)  # 30-day period\n",
    "\n",
    "# Plot decomposition\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(decomposition.observed)\n",
    "plt.title('Observed')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.plot(decomposition.trend)\n",
    "plt.title('Trend')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.plot(decomposition.seasonal)\n",
    "plt.title('Seasonal')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.plot(decomposition.resid)\n",
    "plt.title('Residual')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Feature Importance Preview\n",
    "\n",
    "Let's preview feature importance using a simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare features and target\n",
    "from src.feature_engineering import prepare_features\n",
    "\n",
    "# Prepare features for the first stock\n",
    "target_col = stock_data.columns[0]\n",
    "processed_data, _ = prepare_features(\n",
    "    stock_data,\n",
    "    target_col=target_col,\n",
    "    include_technical=False,  # Set to False since we don't have OHLCV data\n",
    "    include_statistical=True,\n",
    "    include_lags=True,\n",
    "    normalize=False,  # No normalization for feature importance\n",
    "    reduce_dim=False,\n",
    "    forecast_horizon=5\n",
    ")\n",
    "\n",
    "# Drop rows with NaN values\n",
    "processed_data = processed_data.dropna()\n",
    "\n",
    "# Split features and target\n",
    "X = processed_data.drop(columns=[f'Target_5'])\n",
    "y = processed_data[f'Target_5']\n",
    "\n",
    "# Train a Random Forest model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))\n",
    "plt.title('Top 15 Feature Importance')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Conclusion\n",
    "\n",
    "In this notebook, we've explored the stock price dataset and performed initial analysis to understand the data characteristics. We've also previewed some of the technical indicators and feature engineering techniques that we'll use for our LSTM model.\n",
    "\n",
    "Key findings:\n",
    "- The dataset contains multiple stock prices over time\n",
    "- We've analyzed the correlation between different stocks\n",
    "- We've calculated and visualized daily returns and volatility\n",
    "- We've checked for stationarity and performed seasonal decomposition\n",
    "- We've previewed technical indicators and feature importance\n",
    "\n",
    "Next steps:\n",
    "- Implement the full feature engineering pipeline\n",
    "- Prepare sequences for LSTM model\n",
    "- Train and evaluate the LSTM model\n",
    "- Implement the attention mechanism\n",
    "- Evaluate model performance and interpretability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

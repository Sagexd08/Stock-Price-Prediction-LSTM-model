{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation for Stock Price Prediction\n",
    "\n",
    "This notebook demonstrates the process of training and evaluating LSTM models for stock price prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Add parent directory to path to import modules\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data\n",
    "\n",
    "First, let's load the stock price dataset and prepare it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "data_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))), 'data')\n",
    "raw_dir = os.path.join(data_dir, 'raw')\n",
    "processed_dir = os.path.join(data_dir, 'processed')\n",
    "models_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))), 'models')\n",
    "results_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))), 'results')\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Load stock data\n",
    "from src.data_acquisition import load_stock_data\n",
    "\n",
    "stock_data_path = os.path.join(raw_dir, 'stock_data.csv')\n",
    "stock_data = load_stock_data(stock_data_path)\n",
    "\n",
    "# Display the first few rows\n",
    "stock_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "\n",
    "Let's prepare features for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.feature_engineering import prepare_features\n",
    "\n",
    "# Select target column (first stock)\n",
    "target_col = stock_data.columns[0]\n",
    "\n",
    "# Prepare features\n",
    "processed_data, transformers = prepare_features(\n",
    "    stock_data,\n",
    "    target_col=target_col,\n",
    "    include_technical=False,  # Set to False since we don't have OHLCV data\n",
    "    include_statistical=True,\n",
    "    include_lags=True,\n",
    "    normalize=True,\n",
    "    reduce_dim=False,\n",
    "    forecast_horizon=5\n",
    ")\n",
    "\n",
    "# Save processed data\n",
    "processed_data.to_csv(os.path.join(processed_dir, 'processed_stock_data.csv'))\n",
    "\n",
    "# Display the first few rows of processed data\n",
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation for Training\n",
    "\n",
    "Let's prepare the data for training our LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_preparation import prepare_data_for_training\n",
    "\n",
    "# Prepare data for training\n",
    "seq_length = 20\n",
    "forecast_horizon = 1  # Single-step forecasting\n",
    "batch_size = 32\n",
    "\n",
    "train_loader, val_loader, test_loader, feature_dim = prepare_data_for_training(\n",
    "    processed_data,\n",
    "    target_col=f'Target_5',  # Target column created by prepare_features\n",
    "    seq_length=seq_length,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    val_size=0.15,\n",
    "    test_size=0.15,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(f\"Feature dimension: {feature_dim}\")\n",
    "\n",
    "# Check a batch from the training loader\n",
    "for X_batch, y_batch in train_loader:\n",
    "    print(f\"X_batch shape: {X_batch.shape}\")\n",
    "    print(f\"y_batch shape: {y_batch.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Definition\n",
    "\n",
    "Let's define our LSTM models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import create_model\n",
    "\n",
    "# Define model parameters\n",
    "hidden_dim = 64\n",
    "num_layers = 2\n",
    "output_dim = 1  # Single-step forecasting\n",
    "dropout_prob = 0.2\n",
    "\n",
    "# Create models\n",
    "lstm_model = create_model('lstm', feature_dim, hidden_dim, num_layers, output_dim, dropout_prob)\n",
    "lstm_attention_model = create_model('lstm_attention', feature_dim, hidden_dim, num_layers, output_dim, dropout_prob)\n",
    "\n",
    "# Print model architectures\n",
    "print(\"LSTM Model:\")\n",
    "print(lstm_model)\n",
    "print(\"\\nLSTM with Attention Model:\")\n",
    "print(lstm_attention_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "Let's train our LSTM models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train import train_model\n",
    "\n",
    "# Define training parameters\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train LSTM model\n",
    "print(\"Training LSTM model...\")\n",
    "lstm_model.to(device)\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "lstm_model_path = os.path.join(models_dir, 'lstm_model.pth')\n",
    "lstm_history = train_model(\n",
    "    lstm_model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
    "    num_epochs, device, early_stopping_patience=10, model_save_path=lstm_model_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM with Attention model\n",
    "print(\"Training LSTM with Attention model...\")\n",
    "lstm_attention_model.to(device)\n",
    "optimizer = optim.Adam(lstm_attention_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "lstm_attention_model_path = os.path.join(models_dir, 'lstm_attention_model.pth')\n",
    "lstm_attention_history = train_model(\n",
    "    lstm_attention_model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
    "    num_epochs, device, early_stopping_patience=10, model_save_path=lstm_attention_model_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Plot Training History\n",
    "\n",
    "Let's visualize the training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train import plot_training_history\n",
    "\n",
    "# Plot LSTM training history\n",
    "plot_training_history(lstm_history, save_path=os.path.join(results_dir, 'lstm_training_history.png'))\n",
    "\n",
    "# Plot LSTM with Attention training history\n",
    "plot_training_history(lstm_attention_history, save_path=os.path.join(results_dir, 'lstm_attention_training_history.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation\n",
    "\n",
    "Let's evaluate our trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train import evaluate_model, calculate_metrics\n",
    "\n",
    "# Load the best models\n",
    "lstm_model.load_state_dict(torch.load(lstm_model_path, map_location=device))\n",
    "lstm_attention_model.load_state_dict(torch.load(lstm_attention_model_path, map_location=device))\n",
    "\n",
    "# Evaluate LSTM model\n",
    "print(\"Evaluating LSTM model...\")\n",
    "lstm_test_loss, lstm_predictions, lstm_targets = evaluate_model(lstm_model, test_loader, criterion, device)\n",
    "lstm_metrics = calculate_metrics(lstm_predictions, lstm_targets)\n",
    "print(f\"LSTM Test Metrics: {lstm_metrics}\")\n",
    "\n",
    "# Evaluate LSTM with Attention model\n",
    "print(\"\\nEvaluating LSTM with Attention model...\")\n",
    "lstm_attention_test_loss, lstm_attention_predictions, lstm_attention_targets = evaluate_model(lstm_attention_model, test_loader, criterion, device)\n",
    "lstm_attention_metrics = calculate_metrics(lstm_attention_predictions, lstm_attention_targets)\n",
    "print(f\"LSTM with Attention Test Metrics: {lstm_attention_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Predictions\n",
    "\n",
    "Let's visualize the predictions of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluate import plot_predictions, plot_residuals, plot_scatter\n",
    "\n",
    "# Plot LSTM predictions\n",
    "plot_predictions(\n",
    "    lstm_targets, lstm_predictions,\n",
    "    title='LSTM: Actual vs Predicted Values',\n",
    "    save_path=os.path.join(results_dir, 'lstm_predictions.png')\n",
    ")\n",
    "\n",
    "# Plot LSTM with Attention predictions\n",
    "plot_predictions(\n",
    "    lstm_attention_targets, lstm_attention_predictions,\n",
    "    title='LSTM with Attention: Actual vs Predicted Values',\n",
    "    save_path=os.path.join(results_dir, 'lstm_attention_predictions.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals\n",
    "plot_residuals(\n",
    "    lstm_targets, lstm_predictions,\n",
    "    title='LSTM: Residuals Analysis',\n",
    "    save_path=os.path.join(results_dir, 'lstm_residuals.png')\n",
    ")\n",
    "\n",
    "plot_residuals(\n",
    "    lstm_attention_targets, lstm_attention_predictions,\n",
    "    title='LSTM with Attention: Residuals Analysis',\n",
    "    save_path=os.path.join(results_dir, 'lstm_attention_residuals.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scatter plots\n",
    "plot_scatter(\n",
    "    lstm_targets, lstm_predictions,\n",
    "    title='LSTM: Actual vs Predicted Scatter Plot',\n",
    "    save_path=os.path.join(results_dir, 'lstm_scatter.png')\n",
    ")\n",
    "\n",
    "plot_scatter(\n",
    "    lstm_attention_targets, lstm_attention_predictions,\n",
    "    title='LSTM with Attention: Actual vs Predicted Scatter Plot',\n",
    "    save_path=os.path.join(results_dir, 'lstm_attention_scatter.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Uncertainty Quantification\n",
    "\n",
    "Let's quantify the uncertainty in our predictions using Monte Carlo dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluate import monte_carlo_dropout_prediction, plot_prediction_intervals\n",
    "\n",
    "# Get a batch from the test loader\n",
    "for X_batch, y_batch in test_loader:\n",
    "    X_test_sample = X_batch\n",
    "    y_test_sample = y_batch\n",
    "    break\n",
    "\n",
    "# Move to device\n",
    "X_test_sample = X_test_sample.to(device)\n",
    "y_test_sample = y_test_sample.cpu().numpy()\n",
    "\n",
    "# Generate predictions with uncertainty for LSTM model\n",
    "lstm_mean_pred, lstm_std_pred = monte_carlo_dropout_prediction(lstm_model, X_test_sample, n_samples=100, device=device)\n",
    "\n",
    "# Generate predictions with uncertainty for LSTM with Attention model\n",
    "lstm_attention_mean_pred, lstm_attention_std_pred = monte_carlo_dropout_prediction(lstm_attention_model, X_test_sample, n_samples=100, device=device)\n",
    "\n",
    "# Plot prediction intervals\n",
    "plot_prediction_intervals(\n",
    "    y_test_sample, lstm_mean_pred, lstm_std_pred, confidence=0.95,\n",
    "    title='LSTM: Predictions with 95% Confidence Intervals',\n",
    "    save_path=os.path.join(results_dir, 'lstm_prediction_intervals.png')\n",
    ")\n",
    "\n",
    "plot_prediction_intervals(\n",
    "    y_test_sample, lstm_attention_mean_pred, lstm_attention_std_pred, confidence=0.95,\n",
    "    title='LSTM with Attention: Predictions with 95% Confidence Intervals',\n",
    "    save_path=os.path.join(results_dir, 'lstm_attention_prediction_intervals.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Attention Visualization\n",
    "\n",
    "Let's visualize the attention weights to understand which time steps are most important for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluate import visualize_attention\n",
    "\n",
    "# Visualize attention weights\n",
    "visualize_attention(\n",
    "    lstm_attention_model, X_test_sample[:1],\n",
    "    save_path=os.path.join(results_dir, 'attention_weights.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Comprehensive Evaluation\n",
    "\n",
    "Let's perform a comprehensive evaluation of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluate import evaluate_model_comprehensive\n",
    "\n",
    "# Comprehensive evaluation of LSTM model\n",
    "print(\"Performing comprehensive evaluation of LSTM model...\")\n",
    "lstm_output_dir = os.path.join(results_dir, 'lstm')\n",
    "lstm_metrics = evaluate_model_comprehensive(lstm_model, test_loader, device, output_dir=lstm_output_dir)\n",
    "print(f\"LSTM Metrics: {lstm_metrics}\")\n",
    "\n",
    "# Comprehensive evaluation of LSTM with Attention model\n",
    "print(\"\\nPerforming comprehensive evaluation of LSTM with Attention model...\")\n",
    "lstm_attention_output_dir = os.path.join(results_dir, 'lstm_attention')\n",
    "lstm_attention_metrics = evaluate_model_comprehensive(lstm_attention_model, test_loader, device, output_dir=lstm_attention_output_dir)\n",
    "print(f\"LSTM with Attention Metrics: {lstm_attention_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model Comparison\n",
    "\n",
    "Let's compare the performance of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison table\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': list(lstm_metrics.keys()),\n",
    "    'LSTM': list(lstm_metrics.values()),\n",
    "    'LSTM with Attention': list(lstm_attention_metrics.values())\n",
    "})\n",
    "\n",
    "# Display the comparison table\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "metrics = ['MSE', 'RMSE', 'MAE', 'MAPE']\n",
    "lstm_values = [lstm_metrics[metric] for metric in metrics]\n",
    "lstm_attention_values = [lstm_attention_metrics[metric] for metric in metrics]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, lstm_values, width, label='LSTM')\n",
    "plt.bar(x + width/2, lstm_attention_values, width, label='LSTM with Attention')\n",
    "\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Model Comparison')\n",
    "plt.xticks(x, metrics)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, 'model_comparison.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Model Metadata\n",
    "\n",
    "Let's save metadata for our best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Determine the best model based on RMSE\n",
    "if lstm_metrics['RMSE'] < lstm_attention_metrics['RMSE']:\n",
    "    best_model_type = 'lstm'\n",
    "    best_model_path = lstm_model_path\n",
    "    best_metrics = lstm_metrics\n",
    "else:\n",
    "    best_model_type = 'lstm_attention'\n",
    "    best_model_path = lstm_attention_model_path\n",
    "    best_metrics = lstm_attention_metrics\n",
    "\n",
    "# Create metadata\n",
    "metadata = {\n",
    "    'model_type': best_model_type,\n",
    "    'input_dim': feature_dim,\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'num_layers': num_layers,\n",
    "    'output_dim': output_dim,\n",
    "    'dropout_prob': dropout_prob,\n",
    "    'feature_cols': processed_data.drop(columns=[f'Target_5']).columns.tolist(),\n",
    "    'seq_length': seq_length,\n",
    "    'training_date': datetime.now().strftime('%Y%m%d_%H%M%S'),\n",
    "    'test_metrics': best_metrics\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = os.path.join(models_dir, 'model_metadata.json')\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=4)\n",
    "\n",
    "print(f\"Metadata saved to {metadata_path}\")\n",
    "print(f\"Best model: {best_model_type} with RMSE: {best_metrics['RMSE']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Conclusion\n",
    "\n",
    "In this notebook, we've trained and evaluated LSTM models for stock price prediction. We've compared the performance of a basic LSTM model and an LSTM model with attention mechanism.\n",
    "\n",
    "Key findings:\n",
    "- We've prepared features using technical indicators, statistical features, and lag features\n",
    "- We've trained LSTM models with and without attention mechanism\n",
    "- We've evaluated the models using various metrics (RMSE, MAPE, etc.)\n",
    "- We've visualized predictions, residuals, and attention weights\n",
    "- We've quantified prediction uncertainty using Monte Carlo dropout\n",
    "\n",
    "Next steps:\n",
    "- Deploy the best model as an API or dashboard\n",
    "- Implement a retraining pipeline for continuous model updates\n",
    "- Explore multi-step forecasting for longer-term predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
